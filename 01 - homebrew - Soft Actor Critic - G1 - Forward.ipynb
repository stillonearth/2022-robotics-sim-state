{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ssuro\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\tensorboard\\__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n",
      "c:\\Users\\ssuro\\anaconda3\\envs\\torch\\lib\\site-packages\\stable_baselines3\\common\\cmd_util.py:5: FutureWarning: Module ``common.cmd_util`` has been renamed to ``common.env_util`` and will be removed in the future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import progressbar as pb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "\n",
    "from environments import *\n",
    "\n",
    "from stable_baselines3.common.cmd_util import make_vec_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_AGENTS = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ssuro\\anaconda3\\envs\\torch\\lib\\site-packages\\gym\\core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\ssuro\\anaconda3\\envs\\torch\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = make_vec_env('Reacher-v4', n_envs=NUM_AGENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of each action: 2\n",
      "Observation size: 11\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset()\n",
    "\n",
    "# size of each action\n",
    "action_size = env.action_space.shape[0]\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "print('Observation size: {}'.format(state_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact(action):\n",
    "    action = action.reshape(NUM_AGENTS, action_size)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    return next_state.reshape(NUM_AGENTS, -1), np.array(reward).reshape(NUM_AGENTS, -1), np.array(done).reshape(NUM_AGENTS, -1)\n",
    "\n",
    "def reset():\n",
    "    state = env.reset().reshape(NUM_AGENTS, -1)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NET_SIZE = 128\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size=1, n_agents=NUM_AGENTS, fc1_size=NET_SIZE, fc2_size=NET_SIZE):\n",
    "        super(Policy, self).__init__()\n",
    "        \n",
    "        self.bn0 = nn.BatchNorm1d(state_size)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_size)\n",
    "        self.bn1 = nn.BatchNorm1d(fc1_size)\n",
    "        self.fc2 = nn.Linear(fc1_size, fc2_size)\n",
    "        self.bn2 = nn.BatchNorm1d(fc1_size)\n",
    "        self.fc3_mu = nn.Linear(fc2_size, action_size)\n",
    "        self.fc3_std = nn.Linear(fc2_size, action_size)\n",
    "\n",
    "    def forward(self, state, log_std_min=-20, log_std_max=2):\n",
    "        x = self.bn0(state)\n",
    "        x = torch.relu(self.bn1(self.fc1(state)))\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "\n",
    "        mean = self.fc3_mu(x)\n",
    "        std = self.fc3_std(x)\n",
    "        std = torch.clamp(std, log_std_min, log_std_max).exp()\n",
    "\n",
    "        return mean, std\n",
    "    \n",
    "class Value(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size=1, n_agents=NUM_AGENTS, fc1_size=NET_SIZE, fc2_size=NET_SIZE):\n",
    "        super(Value, self).__init__()\n",
    "        \n",
    "        self.bn0 = nn.BatchNorm1d(state_size)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_size)\n",
    "        self.fc2 = nn.Linear(fc1_size, fc2_size)\n",
    "        self.fc3 = nn.Linear(fc2_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn0(x)\n",
    "        x = torch.nn.functional.elu(self.fc1(x))\n",
    "        x = torch.nn.functional.elu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "class Q(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, n_agents=NUM_AGENTS, fc1_size=NET_SIZE, fc2_size=NET_SIZE):\n",
    "        super(Q, self).__init__()      \n",
    "\n",
    "        self.bn0 = nn.BatchNorm1d(state_size+action_size)\n",
    "        self.fc1 = nn.Linear(state_size + action_size, fc1_size)\n",
    "        self.fc2 = nn.Linear(fc1_size, fc2_size)\n",
    "        self.fc3 = nn.Linear(fc2_size, 1)\n",
    "        \n",
    "    def forward(self, s, a):\n",
    "        x = torch.cat([s, a], 1)\n",
    "        x = self.bn0(x)\n",
    "        x = torch.nn.functional.elu(self.fc1(x)) \n",
    "        x = torch.nn.functional.elu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorithms.sac import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    state_size=state_size, \n",
    "    action_size=action_size,\n",
    "    policy_network=Policy,\n",
    "    value_network=Value,\n",
    "    q_network=Q,\n",
    "    n_agents=NUM_AGENTS, \n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.policy_network.load_state_dict(torch.load(\"./trained_models/g1-forward-homebrew/POLICY.pth\"))\n",
    "# agent.value_network_local.load_state_dict(torch.load(\"./trained_models/g1-forward-homebrew/VALUE_TARGET.pth\"))\n",
    "# agent.value_network_target.load_state_dict(torch.load(\"./trained_models/g1-forward-homebrew/VALUE_TARGET.pth\"))\n",
    "# agent.q_network_1.load_state_dict(torch.load(\"./trained_models/g1-forward-homebrew/Q_1.pth\"))\n",
    "# agent.q_network_2.load_state_dict(torch.load(\"./trained_models/g1-forward-homebrew/Q_2.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(n_episodes, t_max, print_every):\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=print_every)  # last 100 scores\n",
    "    widget = ['training loop: ', pb.Percentage(), ' ', pb.Bar(), ' ', pb.ETA()]\n",
    "    timer = pb.ProgressBar(widgets=widget, maxval=n_episodes).start()\n",
    "    \n",
    "    frame_counter = 0\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        states = reset()\n",
    "        score = 0\n",
    "        for t in range(t_max):\n",
    "            frame_counter += 1\n",
    "            actions = agent.act(states)\n",
    "            next_states, rewards, dones = interact(actions)\n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "            states = next_states\n",
    "            score += rewards.mean()\n",
    "            if np.any(dones):\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        agent.writer.add_scalar('score/mean', score, i_episode)\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tScore Mean: {:.2f}\\tScore STD: {:.2f}'.format(i_episode, np.mean(scores_window), np.std(scores_window)))\n",
    "            \n",
    "        timer.update(i_episode)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                           | ETA:  8:12:34\r"
     ]
    }
   ],
   "source": [
    "%time scores = run(t_max=int(1000), n_episodes=int(500), print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_network.state_dict(), \"./trained_models/g1-forward-homebrew/POLICY.pth\")\n",
    "torch.save(agent.value_network_target.state_dict(), \"./trained_models/g1-forward-homebrew/VALUE_TARGET.pth\")\n",
    "torch.save(agent.q_network_1.state_dict(), \"./trained_models/g1-forward-homebrew/Q_1.pth\")\n",
    "torch.save(agent.q_network_2.state_dict(), \"./trained_models/g1-forward-homebrew/Q_2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import RecordVideo\n",
    "\n",
    "env = RecordVideo(gym.make('G1Dist-v0'), './video')\n",
    "state = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "\n",
    "    state = torch.from_numpy(state.reshape(NUM_AGENTS, -1)).float().to(device)\n",
    "\n",
    "    agent.policy_network.eval()\n",
    "    action, _ = agent.sample_action(state)\n",
    "    state, reward, done, info = env.step(action.detach().cpu().numpy())\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "82ea6adb180b12ed72836e614d5d57295654ca2a9780d621124b81b6a9baa809"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
