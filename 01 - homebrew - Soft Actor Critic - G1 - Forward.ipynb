{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.9/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n",
      "/home/user/anaconda3/lib/python3.9/site-packages/stable_baselines3/common/cmd_util.py:5: FutureWarning: Module ``common.cmd_util`` has been renamed to ``common.env_util`` and will be removed in the future.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import progressbar as pb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "\n",
    "from environments import *\n",
    "\n",
    "from stable_baselines3.common.cmd_util import make_vec_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_AGENTS = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/lib/python3.9/site-packages/gym/core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/user/anaconda3/lib/python3.9/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = make_vec_env(\"G1Dist-v1\", n_envs=NUM_AGENTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of each action: 12\n",
      "Observation size: 119\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset()\n",
    "\n",
    "# size of each action\n",
    "action_size = env.action_space.shape[0]\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "print('Observation size: {}'.format(state_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact(action):\n",
    "    action = action.reshape(NUM_AGENTS, action_size)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    return next_state.reshape(NUM_AGENTS, -1), np.array(reward).reshape(NUM_AGENTS, -1), np.array(done).reshape(NUM_AGENTS, -1)\n",
    "\n",
    "def reset():\n",
    "    state = env.reset().reshape(NUM_AGENTS, -1)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NET_SIZE = [512, 512, 512, 512]\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "    state_size, \n",
    "    action_size=1, \n",
    "    net_size=NET_SIZE,\n",
    "    ):\n",
    "        \n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.BatchNorm1d(state_size)\n",
    "        )\n",
    "\n",
    "        for i, size in enumerate(net_size):\n",
    "            prev_size = state_size if i == 0 else net_size[i-1]\n",
    "            self.stack.add_module(f'fc{i}', nn.Linear(prev_size, size))\n",
    "            self.stack.add_module(f'bn{i}', nn.BatchNorm1d(size))\n",
    "            self.stack.add_module(f'elu{i}', nn.ELU())\n",
    "\n",
    "        self.fc_mu = nn.Linear(net_size[-1], action_size)\n",
    "        self.fc_std = nn.Linear(net_size[-1], action_size)\n",
    "\n",
    "    def forward(self, state, log_std_min=-20, log_std_max=2):\n",
    "        x = self.stack(state)\n",
    "\n",
    "        mean = self.fc_mu(x)\n",
    "        std = self.fc_std(x)\n",
    "        std = torch.clamp(std, log_std_min, log_std_max).exp()\n",
    "\n",
    "        return mean, std\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, net_size=NET_SIZE,):\n",
    "        \n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.BatchNorm1d(state_size+action_size)\n",
    "        )\n",
    "\n",
    "        for i, size in enumerate(net_size):            \n",
    "            prev_size = state_size+action_size if i == 0 else net_size[i-1]\n",
    "            self.stack.add_module(f'fc{i}', nn.Linear(prev_size, size))\n",
    "            self.stack.add_module(f'elu{i}', nn.ELU())\n",
    "            \n",
    "        self.stack.add_module(f'fc{i}', nn.Linear(net_size[-1], 1))\n",
    "        \n",
    "    def forward(self, s, a):\n",
    "        x = torch.cat([s, a], 1)\n",
    "        return self.stack(x)\n",
    "\n",
    "class Value(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, net_size=NET_SIZE,):\n",
    "        \n",
    "        super(Value, self).__init__()\n",
    "\n",
    "        self.stack = nn.Sequential(\n",
    "            nn.BatchNorm1d(state_size)\n",
    "        )\n",
    "\n",
    "        for i, size in enumerate(net_size):            \n",
    "            prev_size = state_size if i == 0 else net_size[i-1]\n",
    "            self.stack.add_module(f'fc{i}', nn.Linear(prev_size, size))\n",
    "            self.stack.add_module(f'elu{i}', nn.ELU())\n",
    "            \n",
    "        self.stack.add_module(f'fc{i}', nn.Linear(net_size[-1], 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorithms.sac import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    state_size=state_size, \n",
    "    action_size=action_size,\n",
    "    actor=Actor,\n",
    "    critic=Critic,\n",
    "    value=Value,\n",
    "    n_agents=NUM_AGENTS, \n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.policy_network.load_state_dict(torch.load(\"./trained_models/g1-forward-homebrew/POLICY.pth\"))\n",
    "# agent.value_network_local.load_state_dict(torch.load(\"./trained_models/g1-forward-homebrew/VALUE_TARGET.pth\"))\n",
    "# agent.value_network_target.load_state_dict(torch.load(\"./trained_models/g1-forward-homebrew/VALUE_TARGET.pth\"))\n",
    "# agent.q_network_1.load_state_dict(torch.load(\"./trained_models/g1-forward-homebrew/Q_1.pth\"))\n",
    "# agent.q_network_2.load_state_dict(torch.load(\"./trained_models/g1-forward-homebrew/Q_2.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(n_episodes, t_max, print_every):\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=print_every)  # last 100 scores\n",
    "    widget = ['training loop: ', pb.Percentage(), ' ', pb.Bar(), ' ', pb.ETA()]\n",
    "    timer = pb.ProgressBar(widgets=widget, maxval=n_episodes).start()\n",
    "    \n",
    "    frame_counter = 0\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        states = reset()\n",
    "        score = 0\n",
    "        for t in range(t_max):\n",
    "            frame_counter += 1\n",
    "            actions = agent.act(states)\n",
    "            next_states, rewards, dones = interact(actions)\n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "            states = next_states\n",
    "            score += rewards.mean()\n",
    "            if np.all(dones):\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        agent.writer.add_scalar('score/mean', score, i_episode)\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tScore Mean: {:.2f}\\tScore STD: {:.2f}'.format(i_episode, np.mean(scores_window), np.std(scores_window)))\n",
    "            \n",
    "        timer.update(i_episode)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  10% |########                                                                            | ETA:  3:17:33\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 100\tScore Mean: -379.22\tScore STD: 397.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  20% |################                                                                    | ETA:  2:49:47\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 200\tScore Mean: -1022.43\tScore STD: 671.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  30% |#########################                                                           | ETA:  2:19:45\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 300\tScore Mean: -832.49\tScore STD: 111.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  40% |#################################                                                   | ETA:  2:03:57\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 400\tScore Mean: -646.62\tScore STD: 132.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  50% |##########################################                                          | ETA:  1:45:45\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 500\tScore Mean: -311.55\tScore STD: 60.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  60% |##################################################                                  | ETA:  1:24:52\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 600\tScore Mean: -225.43\tScore STD: 95.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  70% |##########################################################                          | ETA:  1:03:23\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 700\tScore Mean: -29450.21\tScore STD: 28972.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  80% |###################################################################                 | ETA:  0:42:06\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 800\tScore Mean: -37297.72\tScore STD: 12158.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  90% |###########################################################################         | ETA:  0:20:57\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 900\tScore Mean: -50682.80\tScore STD: 4187.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop: 100% |####################################################################################| ETA:  0:00:00\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Episode 1000\tScore Mean: -111068.78\tScore STD: 34410.06\n",
      "CPU times: user 19h 12min 57s, sys: 1h 39min 5s, total: 20h 52min 2s\n",
      "Wall time: 3h 28min 40s\n"
     ]
    }
   ],
   "source": [
    "%time scores = run(t_max=int(100), n_episodes=int(1000), print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure()\n\u001b[1;32m      5\u001b[0m ax \u001b[38;5;241m=\u001b[39m fig\u001b[38;5;241m.\u001b[39madd_subplot(\u001b[38;5;241m111\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(\u001b[43mscores\u001b[49m)), scores)\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpisode #\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scores' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_network.state_dict(), \"./trained_models/g1-forward-homebrew/POLICY.pth\")\n",
    "torch.save(agent.value_network_target.state_dict(), \"./trained_models/g1-forward-homebrew/VALUE_TARGET.pth\")\n",
    "torch.save(agent.q_network_1.state_dict(), \"./trained_models/g1-forward-homebrew/Q_1.pth\")\n",
    "torch.save(agent.q_network_2.state_dict(), \"./trained_models/g1-forward-homebrew/Q_2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import RecordVideo\n",
    "\n",
    "env = RecordVideo(gym.make('G1Dist-v0'), './video')\n",
    "state = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "\n",
    "    state = torch.from_numpy(state.reshape(NUM_AGENTS, -1)).float().to(device)\n",
    "\n",
    "    agent.policy_network.eval()\n",
    "    action, _ = agent.sample_action(state)\n",
    "    state, reward, done, info = env.step(action.detach().cpu().numpy())\n",
    "    \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "82ea6adb180b12ed72836e614d5d57295654ca2a9780d621124b81b6a9baa809"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
