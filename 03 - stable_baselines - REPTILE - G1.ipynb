{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from environments import *\n",
    "\n",
    "from stable_baselines3 import SAC, HerReplayBuffer\n",
    "from stable_baselines3.common.cmd_util import make_vec_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_vec_env(\"G1GoalDist-v1\", 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "model = SAC(\n",
    "    \"MlpPolicy\", \n",
    "    env, \n",
    "    verbose=1,\n",
    "    policy_kwargs=dict(\n",
    "        net_arch=[512, 512, 512, 512], \n",
    "        activation_fn=torch.nn.ELU,\n",
    "        n_critics=2,\n",
    "    ),\n",
    "    batch_size=2048,\n",
    "    learning_rate=1e-4,\n",
    "    # learning_starts=1000,\n",
    "    tensorboard_log=\"./logs/sac/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def soft_update(task_model, meta_model, tau=0.2):\n",
    "    for target_param, local_param in zip(meta_model.parameters(), task_model.parameters()):\n",
    "        target_param.data.copy_(target_param.data + tau * (local_param.data-target_param.data))\n",
    "\n",
    "def reptile(n_reptile_iterations=100000, n_sac_iterations=50000):\n",
    "    \n",
    "    meta_policy = copy.copy(model.policy)\n",
    "    tasks = model.env.env_method('sample_tasks', (n_reptile_iterations), indices=0)[0]\n",
    "\n",
    "    for task in tasks:\n",
    "\n",
    "        print(\"Task: \", task)\n",
    "                \n",
    "        model.env.env_method('reset_task', (task))\n",
    "        model.policy = copy.copy(meta_policy)\n",
    "        # model.replay_buffer.reset()\n",
    "\n",
    "        model.learn(\n",
    "            total_timesteps=n_sac_iterations, \n",
    "            log_interval=100,\n",
    "            tb_log_name='reptile',\n",
    "            reset_num_timesteps=False,\n",
    "        )\n",
    "\n",
    "        soft_update(model.policy, meta_policy, 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task:  {'direction': 4.39669994022792, 'orienation': 4.280881071181969}\n",
      "Logging to ./logs/sac/reptile_0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '__dir__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mreptile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [9], line 20\u001b[0m, in \u001b[0;36mreptile\u001b[1;34m(n_reptile_iterations, n_sac_iterations)\u001b[0m\n\u001b[0;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(meta_policy)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# model.replay_buffer.reset()\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_sac_iterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreptile\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m soft_update(model\u001b[38;5;241m.\u001b[39mpolicy, meta_policy, \u001b[38;5;241m0.2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ssuro\\anaconda3\\envs\\torch\\lib\\site-packages\\stable_baselines3\\sac\\sac.py:298\u001b[0m, in \u001b[0;36mSAC.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    286\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    287\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    295\u001b[0m     reset_num_timesteps: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    296\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m OffPolicyAlgorithm:\n\u001b[1;32m--> 298\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m    299\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[0;32m    300\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    301\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    302\u001b[0m         eval_env\u001b[39m=\u001b[39;49meval_env,\n\u001b[0;32m    303\u001b[0m         eval_freq\u001b[39m=\u001b[39;49meval_freq,\n\u001b[0;32m    304\u001b[0m         n_eval_episodes\u001b[39m=\u001b[39;49mn_eval_episodes,\n\u001b[0;32m    305\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[0;32m    306\u001b[0m         eval_log_path\u001b[39m=\u001b[39;49meval_log_path,\n\u001b[0;32m    307\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[0;32m    308\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ssuro\\anaconda3\\envs\\torch\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:346\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    343\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[0;32m    345\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 346\u001b[0m     rollout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\n\u001b[0;32m    347\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv,\n\u001b[0;32m    348\u001b[0m         train_freq\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_freq,\n\u001b[0;32m    349\u001b[0m         action_noise\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_noise,\n\u001b[0;32m    350\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    351\u001b[0m         learning_starts\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearning_starts,\n\u001b[0;32m    352\u001b[0m         replay_buffer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreplay_buffer,\n\u001b[0;32m    353\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    354\u001b[0m     )\n\u001b[0;32m    356\u001b[0m     \u001b[39mif\u001b[39;00m rollout\u001b[39m.\u001b[39mcontinue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m    357\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ssuro\\anaconda3\\envs\\torch\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:579\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[0;32m    576\u001b[0m actions, buffer_actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[39m.\u001b[39mnum_envs)\n\u001b[0;32m    578\u001b[0m \u001b[39m# Rescale and perform action\u001b[39;00m\n\u001b[1;32m--> 579\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(actions)\n\u001b[0;32m    581\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[0;32m    582\u001b[0m num_collected_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ssuro\\anaconda3\\envs\\torch\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:162\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \n\u001b[0;32m    158\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[1;32mc:\\Users\\ssuro\\anaconda3\\envs\\torch\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:43\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     42\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[1;32m---> 43\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[0;32m     44\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[0;32m     45\u001b[0m         )\n\u001b[0;32m     46\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx]:\n\u001b[0;32m     47\u001b[0m             \u001b[39m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[0;32m     48\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx][\u001b[39m\"\u001b[39m\u001b[39mterminal_observation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m obs\n",
      "File \u001b[1;32mc:\\Users\\ssuro\\anaconda3\\envs\\torch\\lib\\site-packages\\stable_baselines3\\common\\monitor.py:90\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[0;32m     89\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 90\u001b[0m observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     91\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[0;32m     92\u001b[0m \u001b[39mif\u001b[39;00m done:\n",
      "File \u001b[1;32mc:\\Users\\ssuro\\anaconda3\\envs\\torch\\lib\\site-packages\\gym\\wrappers\\time_limit.py:60\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     49\u001b[0m     \u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \n\u001b[0;32m     51\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39m        \"TimeLimit.truncated\"=False if the environment terminated\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m     59\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m step_api_compatibility(\n\u001b[1;32m---> 60\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action),\n\u001b[0;32m     61\u001b[0m         \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     62\u001b[0m     )\n\u001b[0;32m     63\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\ssuro\\anaconda3\\envs\\torch\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     36\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\ssuro\\anaconda3\\envs\\torch\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:52\u001b[0m, in \u001b[0;36mStepAPICompatibility.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     44\u001b[0m     \u001b[39m\"\"\"Steps through the environment, returning 5 or 4 items depending on `new_step_api`.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \n\u001b[0;32m     46\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39m        (observation, reward, terminated, truncated, info) or (observation, reward, done, info)\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m     step_returns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnew_step_api:\n\u001b[0;32m     54\u001b[0m         \u001b[39mreturn\u001b[39;00m step_to_new_api(step_returns)\n",
      "File \u001b[1;32mc:\\Users\\ssuro\\anaconda3\\envs\\torch\\lib\\site-packages\\gym\\wrappers\\env_checker.py:37\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchecked_step \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchecked_step \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, action)\n\u001b[0;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\ssuro\\anaconda3\\envs\\torch\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:222\u001b[0m, in \u001b[0;36menv_step_passive_checker\u001b[1;34m(env, action)\u001b[0m\n\u001b[0;32m    220\u001b[0m \u001b[39m\"\"\"A passive check for the environment step, investigating the returning data then returning the data unchanged.\"\"\"\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[39m# We don't check the action as for some environments then out-of-bounds values can be given\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m result \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m    223\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[0;32m    224\u001b[0m     result, \u001b[39mtuple\u001b[39m\n\u001b[0;32m    225\u001b[0m ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpects step result to be a tuple, actual type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(result)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    226\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(result) \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ssuro\\git\\continuous_control-unitree-g1\\environments\\g1.py:366\u001b[0m, in \u001b[0;36mG1GoalDistanceEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    361\u001b[0m goal_direction \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\n\u001b[0;32m    362\u001b[0m     [np\u001b[39m.\u001b[39mcos(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_goal_dir), np\u001b[39m.\u001b[39msin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_goal_dir)])\n\u001b[0;32m    363\u001b[0m projected_speed \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(xy_velocity, goal_direction \u001b[39m*\u001b[39m\n\u001b[0;32m    364\u001b[0m                          abs_velocity) \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(goal_direction)\n\u001b[1;32m--> 366\u001b[0m body_orientation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_body_orientation(\u001b[39m\"\u001b[39;49m\u001b[39mtrunk\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    367\u001b[0m goal_orientation \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\n\u001b[0;32m    368\u001b[0m     [np\u001b[39m.\u001b[39mcos(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_goal_dir), np\u001b[39m.\u001b[39msin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_goal_dir)])\n\u001b[0;32m    369\u001b[0m goal_orientation \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(\n\u001b[0;32m    370\u001b[0m     body_orientation[:\u001b[39m2\u001b[39m], goal_orientation) \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(goal_orientation)\n",
      "File \u001b[1;32mc:\\Users\\ssuro\\git\\continuous_control-unitree-g1\\environments\\g1.py:338\u001b[0m, in \u001b[0;36mG1GoalDistanceEnv._get_body_orientation\u001b[1;34m(self, body_name)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_body_orientation\u001b[39m(\u001b[39mself\u001b[39m, body_name: \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> 338\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m__dir__\u001b[39;49m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata))\n\u001b[0;32m    339\u001b[0m     now_quat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mget_body_xquat(body_name)\n\u001b[0;32m    341\u001b[0m     res \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39m4\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name '__dir__' is not defined"
     ]
    }
   ],
   "source": [
    "reptile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ssuro\\anaconda3\\envs\\torch\\lib\\site-packages\\gym\\core.py:329: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "c:\\Users\\ssuro\\anaconda3\\envs\\torch\\lib\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"G1GoalDist-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_MjDataBodyViews\n",
       "  cacc: array([0., 0., 0., 0., 0., 0.])\n",
       "  cfrc_ext: array([0., 0., 0., 0., 0., 0.])\n",
       "  cfrc_int: array([0., 0., 0., 0., 0., 0.])\n",
       "  cinert: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "  crb: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "  cvel: array([0., 0., 0., 0., 0., 0.])\n",
       "  subtree_angmom: array([0., 0., 0.])\n",
       "  subtree_com: array([0., 0., 0.])\n",
       "  subtree_linvel: array([0., 0., 0.])\n",
       "  xfrc_applied: array([0., 0., 0., 0., 0., 0.])\n",
       "  ximat: array([0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "  xipos: array([0., 0., 0.])\n",
       "  xmat: array([0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
       "  xpos: array([0., 0., 0.])\n",
       "  xquat: array([0., 0., 0., 0.])\n",
       ">"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "82ea6adb180b12ed72836e614d5d57295654ca2a9780d621124b81b6a9baa809"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
