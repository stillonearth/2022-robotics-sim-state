{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import progressbar as pb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "\n",
    "from environments import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('G1Dist-v0', new_step_api=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of each action: 12\n",
      "Observation size: 35\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset()\n",
    "\n",
    "# size of each action\n",
    "action_size = env.action_space.shape[0]\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "print('Observation size: {}'.format(state_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_AGENTS = 1\n",
    "\n",
    "def interact(action):\n",
    "    action = action.reshape(NUM_AGENTS, action_size)\n",
    "    next_state, reward, done, trunc, info = env.step(action)\n",
    "    return next_state.reshape(NUM_AGENTS, -1), np.array(reward).reshape(NUM_AGENTS, -1), np.array(done or trunc).reshape(NUM_AGENTS, -1)\n",
    "\n",
    "def reset():\n",
    "    state = env.reset().reshape(NUM_AGENTS, -1)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NET_SIZE = 512\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size=1, n_agents=NUM_AGENTS, fc1_size=NET_SIZE, fc2_size=NET_SIZE):\n",
    "        super(Policy, self).__init__()\n",
    "        \n",
    "        self.bn0 = nn.BatchNorm1d(state_size)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_size)\n",
    "        self.bn1 = nn.BatchNorm1d(fc1_size)\n",
    "        self.fc2 = nn.Linear(fc1_size, fc2_size)\n",
    "        self.bn2 = nn.BatchNorm1d(fc1_size)\n",
    "        self.fc3_mu = nn.Linear(fc2_size, action_size)\n",
    "        self.fc3_std = nn.Linear(fc2_size, action_size)\n",
    "\n",
    "    def forward(self, state, log_std_min=-20, log_std_max=2):\n",
    "        x = self.bn0(state)\n",
    "        x = torch.relu(self.bn1(self.fc1(state)))\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "\n",
    "        mean = self.fc3_mu(x)\n",
    "        std = self.fc3_std(x)\n",
    "        std = torch.clamp(std, log_std_min, log_std_max).exp()\n",
    "\n",
    "        return mean, std\n",
    "    \n",
    "class Value(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size=1, n_agents=NUM_AGENTS, fc1_size=NET_SIZE, fc2_size=NET_SIZE):\n",
    "        \n",
    "        super(Value, self).__init__()\n",
    "        \n",
    "        self.bn0 = nn.BatchNorm1d(state_size)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_size)\n",
    "        self.fc2 = nn.Linear(fc1_size, fc2_size)\n",
    "        self.fc3 = nn.Linear(fc2_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn0(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "class Q(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, n_agents=NUM_AGENTS, fc1_size=NET_SIZE, fc2_size=NET_SIZE):\n",
    "        \n",
    "        super(Q, self).__init__()      \n",
    "\n",
    "        self.bn0 = nn.BatchNorm1d(state_size+action_size)\n",
    "        self.fc1 = nn.Linear(state_size + action_size, fc1_size)\n",
    "        self.fc2 = nn.Linear(fc1_size, fc2_size)\n",
    "        self.fc3 = nn.Linear(fc2_size, 1)\n",
    "        \n",
    "    def forward(self, s, a):\n",
    "        x = torch.cat([s, a], 1)\n",
    "        x = self.bn0(x)\n",
    "        x = torch.relu(self.fc1(x)) \n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sergei\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\tensorboard\\__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n"
     ]
    }
   ],
   "source": [
    "from algorithms.sac import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    state_size=state_size, \n",
    "    action_size=action_size,\n",
    "    policy_network=Policy,\n",
    "    value_network=Value,\n",
    "    q_network=Q,\n",
    "    n_agents=NUM_AGENTS, \n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.policy_network.load_state_dict(torch.load(\"./trained_models/g1-forward/POLICY.pth\"))\n",
    "# agent.value_network_local.load_state_dict(torch.load(\"./trained_models/g1-forward/VALUE_TARGET.pth\"))\n",
    "# agent.value_network_target.load_state_dict(torch.load(\"./trained_models/g1-forward/VALUE_TARGET.pth\"))\n",
    "# agent.q_network_1.load_state_dict(torch.load(\"./trained_models/g1-forward/Q_1.pth\"))\n",
    "# agent.q_network_2.load_state_dict(torch.load(\"./trained_models/g1-forward/Q_2.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sergei\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\gym\\core.py:57: DeprecationWarning: \u001b[33mWARN: You are calling render method, but you didn't specified the argument render_mode at environment initialization. To maintain backward compatibility, the environment will render in human mode.\n",
      "If you want to render in human mode, initialize the environment in this way: gym.make('EnvName', render_mode='human') and don't call the render method.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(n_episodes, t_max, print_every):\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=print_every)  # last 100 scores\n",
    "    widget = ['training loop: ', pb.Percentage(), ' ', pb.Bar(), ' ', pb.ETA()]\n",
    "    timer = pb.ProgressBar(widgets=widget, maxval=n_episodes).start()\n",
    "    \n",
    "    frame_counter = 0\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        states = reset()\n",
    "        score = 0\n",
    "        for t in range(t_max):\n",
    "            frame_counter += 1\n",
    "            actions = agent.act(states)\n",
    "            next_states, rewards, dones = interact(actions)\n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "            states = next_states\n",
    "            score += rewards.mean()\n",
    "            if np.any(dones):\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        agent.writer.add_scalar('score/mean', score, i_episode)\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tScore Mean: {:.2f}\\tScore STD: {:.2f}'.format(i_episode, np.mean(scores_window), np.std(scores_window)))\n",
    "            \n",
    "        timer.update(i_episode)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   2% |#                                          | ETA:  0:21:05\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 30\tScore Mean: -51.05\tScore STD: 58.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   5% |##                                         | ETA:  0:18:44\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 60\tScore Mean: -43.10\tScore STD: 34.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   9% |###                                        | ETA:  0:17:46\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 90\tScore Mean: -8.29\tScore STD: 14.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  12% |#####                                      | ETA:  0:17:55\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 120\tScore Mean: -19.78\tScore STD: 21.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  15% |######                                     | ETA:  0:17:44\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 150\tScore Mean: -30.88\tScore STD: 26.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  18% |#######                                    | ETA:  0:17:21\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 180\tScore Mean: -14.66\tScore STD: 20.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  18% |#######                                    | ETA:  0:17:23\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Sergei\\git\\walking-robot-neural-control\\2022-robotics-sim-state\\Soft Actor Critic - G1 - Forward.ipynb Cell 10\u001b[0m in \u001b[0;36mrun\u001b[1;34m(n_episodes, t_max, print_every)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergei/git/walking-robot-neural-control/2022-robotics-sim-state/Soft%20Actor%20Critic%20-%20G1%20-%20Forward.ipynb#ch0000009?line=13'>14</a>\u001b[0m actions \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mact(states)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergei/git/walking-robot-neural-control/2022-robotics-sim-state/Soft%20Actor%20Critic%20-%20G1%20-%20Forward.ipynb#ch0000009?line=14'>15</a>\u001b[0m next_states, rewards, dones \u001b[39m=\u001b[39m interact(actions)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Sergei/git/walking-robot-neural-control/2022-robotics-sim-state/Soft%20Actor%20Critic%20-%20G1%20-%20Forward.ipynb#ch0000009?line=15'>16</a>\u001b[0m agent\u001b[39m.\u001b[39;49mstep(states, actions, rewards, next_states, dones)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergei/git/walking-robot-neural-control/2022-robotics-sim-state/Soft%20Actor%20Critic%20-%20G1%20-%20Forward.ipynb#ch0000009?line=16'>17</a>\u001b[0m states \u001b[39m=\u001b[39m next_states\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergei/git/walking-robot-neural-control/2022-robotics-sim-state/Soft%20Actor%20Critic%20-%20G1%20-%20Forward.ipynb#ch0000009?line=17'>18</a>\u001b[0m score \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m rewards\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[1;32mc:\\Users\\Sergei\\git\\walking-robot-neural-control\\2022-robotics-sim-state\\algorithms\\sac.py:138\u001b[0m, in \u001b[0;36mAgent.step\u001b[1;34m(self, states, actions, rewards, next_states, dones)\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    137\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_counter \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m TARGET_UPDATE_INTERVAL \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39mmemory) \u001b[39m>\u001b[39m BATCH_SIZE:\n\u001b[1;32m--> 138\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearn()\n\u001b[0;32m    139\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_counter \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Sergei\\git\\walking-robot-neural-control\\2022-robotics-sim-state\\algorithms\\sac.py:73\u001b[0m, in \u001b[0;36mAgent.learn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     70\u001b[0m states, actions, rewards, next_states, dones \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39msample(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     72\u001b[0m \u001b[39m# Calculate V and Q targets\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m y_q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalue_q(states, next_states, rewards, dones, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalue_network_target)\u001b[39m.\u001b[39mdetach()\n\u001b[0;32m     74\u001b[0m y_v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue_v(states)\n\u001b[0;32m     76\u001b[0m \u001b[39m# Update V-function\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sergei\\git\\walking-robot-neural-control\\2022-robotics-sim-state\\algorithms\\sac.py:53\u001b[0m, in \u001b[0;36mAgent.value_q\u001b[1;34m(self, states, next_states, rewards, dones, network, gamma)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalue_q\u001b[39m(\u001b[39mself\u001b[39m, states, next_states, rewards, dones, network, gamma\u001b[39m=\u001b[39mGAMMA):\n\u001b[1;32m---> 53\u001b[0m     \u001b[39mreturn\u001b[39;00m (rewards \u001b[39m+\u001b[39m gamma \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m dones) \u001b[39m*\u001b[39m network(next_states))\n",
      "File \u001b[1;32mc:\\Users\\Sergei\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\Sergei\\git\\walking-robot-neural-control\\2022-robotics-sim-state\\Soft Actor Critic - G1 - Forward.ipynb Cell 10\u001b[0m in \u001b[0;36mValue.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergei/git/walking-robot-neural-control/2022-robotics-sim-state/Soft%20Actor%20Critic%20-%20G1%20-%20Forward.ipynb#ch0000009?line=38'>39</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn0(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergei/git/walking-robot-neural-control/2022-robotics-sim-state/Soft%20Actor%20Critic%20-%20G1%20-%20Forward.ipynb#ch0000009?line=39'>40</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(x))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Sergei/git/walking-robot-neural-control/2022-robotics-sim-state/Soft%20Actor%20Critic%20-%20G1%20-%20Forward.ipynb#ch0000009?line=40'>41</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc2(x))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergei/git/walking-robot-neural-control/2022-robotics-sim-state/Soft%20Actor%20Critic%20-%20G1%20-%20Forward.ipynb#ch0000009?line=41'>42</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc3(x)\n",
      "File \u001b[1;32mc:\\Users\\Sergei\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Sergei\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%time scores = run(t_max=int(10000), n_episodes=int(5000), print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_network.state_dict(), \"./trained_models/g1-forward/POLICY.pth\")\n",
    "torch.save(agent.value_network_target.state_dict(), \"./trained_models/g1-forward/VALUE_TARGET.pth\")\n",
    "torch.save(agent.q_network_1.state_dict(), \"./trained_models/g1-forward/Q_1.pth\")\n",
    "torch.save(agent.q_network_2.state_dict(), \"./trained_models/g1-forward/Q_2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "74d63dd06fb5b9584c5a99e0f23a7a9506726cecc90dfceeb064a9b990e8da94"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
