{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import progressbar as pb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "\n",
    "from environments import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('G1Dist-v0', new_step_api=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of each action: 12\n",
      "Observation size: 119\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset()\n",
    "\n",
    "# size of each action\n",
    "action_size = env.action_space.shape[0]\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "print('Observation size: {}'.format(state_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_AGENTS = 1\n",
    "\n",
    "def interact(action):\n",
    "    action = action.reshape(NUM_AGENTS, action_size)\n",
    "    next_state, reward, done, trunc, info = env.step(action)\n",
    "    return next_state.reshape(NUM_AGENTS, -1), np.array(reward).reshape(NUM_AGENTS, -1), np.array(done or trunc).reshape(NUM_AGENTS, -1)\n",
    "\n",
    "def reset():\n",
    "    state = env.reset().reshape(NUM_AGENTS, -1)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "NET_SIZE = 512\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size=1, n_agents=NUM_AGENTS, fc1_size=NET_SIZE, fc2_size=NET_SIZE):\n",
    "        super(Policy, self).__init__()\n",
    "        \n",
    "        self.bn0 = nn.BatchNorm1d(state_size)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_size)\n",
    "        self.bn1 = nn.BatchNorm1d(fc1_size)\n",
    "        self.fc2 = nn.Linear(fc1_size, fc2_size)\n",
    "        self.bn2 = nn.BatchNorm1d(fc1_size)\n",
    "        self.fc3_mu = nn.Linear(fc2_size, action_size)\n",
    "        self.fc3_std = nn.Linear(fc2_size, action_size)\n",
    "\n",
    "    def forward(self, state, log_std_min=-20, log_std_max=2):\n",
    "        x = self.bn0(state)\n",
    "        x = torch.relu(self.bn1(self.fc1(state)))\n",
    "        x = torch.relu(self.bn2(self.fc2(x)))\n",
    "\n",
    "        mean = self.fc3_mu(x)\n",
    "        std = self.fc3_std(x)\n",
    "        std = torch.clamp(std, log_std_min, log_std_max).exp()\n",
    "\n",
    "        return mean, std\n",
    "    \n",
    "class Value(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size=1, n_agents=NUM_AGENTS, fc1_size=NET_SIZE, fc2_size=NET_SIZE):\n",
    "        \n",
    "        super(Value, self).__init__()\n",
    "        \n",
    "        self.bn0 = nn.BatchNorm1d(state_size)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_size)\n",
    "        self.fc2 = nn.Linear(fc1_size, fc2_size)\n",
    "        self.fc3 = nn.Linear(fc2_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn0(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "class Q(nn.Module):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, n_agents=NUM_AGENTS, fc1_size=NET_SIZE, fc2_size=NET_SIZE):\n",
    "        \n",
    "        super(Q, self).__init__()      \n",
    "\n",
    "        self.bn0 = nn.BatchNorm1d(state_size+action_size)\n",
    "        self.fc1 = nn.Linear(state_size + action_size, fc1_size)\n",
    "        self.fc2 = nn.Linear(fc1_size, fc2_size)\n",
    "        self.fc3 = nn.Linear(fc2_size, 1)\n",
    "        \n",
    "    def forward(self, s, a):\n",
    "        x = torch.cat([s, a], 1)\n",
    "        x = self.bn0(x)\n",
    "        x = torch.relu(self.fc1(x)) \n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorithms.sac import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    state_size=state_size, \n",
    "    action_size=action_size,\n",
    "    policy_network=Policy,\n",
    "    value_network=Value,\n",
    "    q_network=Q,\n",
    "    n_agents=NUM_AGENTS, \n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.policy_network.load_state_dict(torch.load(\"./trained_models/g1-forward/POLICY.pth\"))\n",
    "# agent.value_network_local.load_state_dict(torch.load(\"./trained_models/g1-forward/VALUE_TARGET.pth\"))\n",
    "# agent.value_network_target.load_state_dict(torch.load(\"./trained_models/g1-forward/VALUE_TARGET.pth\"))\n",
    "# agent.q_network_1.load_state_dict(torch.load(\"./trained_models/g1-forward/Q_1.pth\"))\n",
    "# agent.q_network_2.load_state_dict(torch.load(\"./trained_models/g1-forward/Q_2.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(n_episodes, t_max, print_every):\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=print_every)  # last 100 scores\n",
    "    widget = ['training loop: ', pb.Percentage(), ' ', pb.Bar(), ' ', pb.ETA()]\n",
    "    timer = pb.ProgressBar(widgets=widget, maxval=n_episodes).start()\n",
    "    \n",
    "    frame_counter = 0\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        states = reset()\n",
    "        score = 0\n",
    "        for t in range(t_max):\n",
    "            frame_counter += 1\n",
    "            actions = agent.act(states)\n",
    "            next_states, rewards, dones = interact(actions)\n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "            states = next_states\n",
    "            score += rewards.mean()\n",
    "            if np.any(dones):\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        agent.writer.add_scalar('score/mean', score, i_episode)\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode {}\\tScore Mean: {:.2f}\\tScore STD: {:.2f}'.format(i_episode, np.mean(scores_window), np.std(scores_window)))\n",
    "            \n",
    "        timer.update(i_episode)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   0% |                                          | ETA:  --:--:--\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00164155 -0.06633493  0.07490799 -0.02165997  0.08627363 -0.07351102\n",
      "   0.04594614  0.001915    0.08809693 -0.12456954 -0.11805056  0.02438952]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 119 and the array at index 151 has size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Sergei\\git\\walking-robot-neural-control\\2022-robotics-sim-state\\Soft Actor Critic - G1 - Forward.ipynb Cell 10\u001b[0m in \u001b[0;36mrun\u001b[1;34m(n_episodes, t_max, print_every)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergei/git/walking-robot-neural-control/2022-robotics-sim-state/Soft%20Actor%20Critic%20-%20G1%20-%20Forward.ipynb#ch0000009?line=14'>15</a>\u001b[0m next_states, rewards, dones \u001b[39m=\u001b[39m interact(actions)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergei/git/walking-robot-neural-control/2022-robotics-sim-state/Soft%20Actor%20Critic%20-%20G1%20-%20Forward.ipynb#ch0000009?line=15'>16</a>\u001b[0m \u001b[39mprint\u001b[39m(actions)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Sergei/git/walking-robot-neural-control/2022-robotics-sim-state/Soft%20Actor%20Critic%20-%20G1%20-%20Forward.ipynb#ch0000009?line=16'>17</a>\u001b[0m agent\u001b[39m.\u001b[39;49mstep(states, actions, rewards, next_states, dones)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergei/git/walking-robot-neural-control/2022-robotics-sim-state/Soft%20Actor%20Critic%20-%20G1%20-%20Forward.ipynb#ch0000009?line=17'>18</a>\u001b[0m states \u001b[39m=\u001b[39m next_states\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Sergei/git/walking-robot-neural-control/2022-robotics-sim-state/Soft%20Actor%20Critic%20-%20G1%20-%20Forward.ipynb#ch0000009?line=18'>19</a>\u001b[0m score \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m rewards\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[1;32mc:\\Users\\Sergei\\git\\walking-robot-neural-control\\2022-robotics-sim-state\\algorithms\\sac.py:129\u001b[0m, in \u001b[0;36mAgent.step\u001b[1;34m(self, states, actions, rewards, next_states, dones)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_counter \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m TARGET_UPDATE_INTERVAL \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39mmemory) \u001b[39m>\u001b[39m BATCH_SIZE:\n\u001b[1;32m--> 129\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearn()\n\u001b[0;32m    130\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_counter \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Sergei\\git\\walking-robot-neural-control\\2022-robotics-sim-state\\algorithms\\sac.py:75\u001b[0m, in \u001b[0;36mAgent.learn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m     73\u001b[0m     \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(GRADIENT_STEPS):\n\u001b[1;32m---> 75\u001b[0m         states, actions, rewards, next_states, dones \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmemory\u001b[39m.\u001b[39;49msample(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[0;32m     77\u001b[0m         \u001b[39m# Calculate V and Q targets\u001b[39;00m\n\u001b[0;32m     78\u001b[0m         y_q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue_q(states, next_states, rewards, dones, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue_network_target)\u001b[39m.\u001b[39mdetach()\n",
      "File \u001b[1;32mc:\\Users\\Sergei\\git\\walking-robot-neural-control\\2022-robotics-sim-state\\algorithms\\sac.py:179\u001b[0m, in \u001b[0;36mReplayBuffer.sample\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[39m\"\"\"Randomly sample a batch of experiences from memory.\"\"\"\u001b[39;00m\n\u001b[0;32m    176\u001b[0m experiences \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39msample(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory, k\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size)\n\u001b[0;32m    178\u001b[0m states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(\n\u001b[1;32m--> 179\u001b[0m     np\u001b[39m.\u001b[39;49mvstack([e\u001b[39m.\u001b[39;49mstate \u001b[39mfor\u001b[39;49;00m e \u001b[39min\u001b[39;49;00m experiences \u001b[39mif\u001b[39;49;00m e \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m]))\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m    180\u001b[0m actions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(\n\u001b[0;32m    181\u001b[0m     np\u001b[39m.\u001b[39mvstack([e\u001b[39m.\u001b[39maction \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m experiences \u001b[39mif\u001b[39;00m e \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m]))\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m    182\u001b[0m rewards \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(\n\u001b[0;32m    183\u001b[0m     np\u001b[39m.\u001b[39mvstack([e\u001b[39m.\u001b[39mreward \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m experiences \u001b[39mif\u001b[39;00m e \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m]))\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Sergei\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\numpy\\core\\shape_base.py:282\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(arrs, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    281\u001b[0m     arrs \u001b[39m=\u001b[39m [arrs]\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m _nx\u001b[39m.\u001b[39;49mconcatenate(arrs, \u001b[39m0\u001b[39;49m)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 119 and the array at index 151 has size 1"
     ]
    }
   ],
   "source": [
    "%time scores = run(t_max=int(10000), n_episodes=int(500), print_every=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_network.state_dict(), \"./trained_models/g1-forward/POLICY.pth\")\n",
    "torch.save(agent.value_network_target.state_dict(), \"./trained_models/g1-forward/VALUE_TARGET.pth\")\n",
    "torch.save(agent.q_network_1.state_dict(), \"./trained_models/g1-forward/Q_1.pth\")\n",
    "torch.save(agent.q_network_2.state_dict(), \"./trained_models/g1-forward/Q_2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "74d63dd06fb5b9584c5a99e0f23a7a9506726cecc90dfceeb064a9b990e8da94"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
